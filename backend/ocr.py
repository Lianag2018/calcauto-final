"""
OCR Pipeline Industriel - OpenCV + Google Cloud Vision
Version optimis√©e pour factures FCA Canada

Pipeline:
Image ‚Üí Correction Perspective ‚Üí Pr√©traitement CamScanner ‚Üí OCR Google Vision ‚Üí Parsing

Architecture:
- Niveau 1: PDF natif ‚Üí pdfplumber (100% pr√©cision)
- Niveau 2: Images ‚Üí OpenCV pr√©traitement + Tesseract (85-92% pr√©cision)  
- Niveau 3: Fallback ‚Üí Google Cloud Vision (si score < 70) - plus pr√©cis et moins cher que GPT-4
"""

import cv2
import numpy as np
import pytesseract
from PIL import Image
import io
import logging
import os
import base64
import requests
from typing import Dict, Tuple, Optional

logger = logging.getLogger(__name__)

# ============ GOOGLE CLOUD VISION OCR ============

GOOGLE_VISION_ENDPOINT = "https://vision.googleapis.com/v1/images:annotate"


def google_vision_ocr(image_base64: str, api_key: Optional[str] = None) -> Dict[str, any]:
    """
    Appel √† Google Cloud Vision API pour OCR.
    
    Utilise DOCUMENT_TEXT_DETECTION qui est optimis√© pour les documents
    et l'√©criture manuscrite (meilleur que TEXT_DETECTION pour les factures).
    
    Args:
        image_base64: Image encod√©e en base64
        api_key: Cl√© API Google Cloud Vision (optionnelle, prend de l'env si non fournie)
    
    Returns:
        Dict avec:
        - full_text: Texte complet d√©tect√©
        - success: True si la requ√™te a r√©ussi
        - confidence: Score de confiance moyen
        - error: Message d'erreur si √©chec
    """
    if api_key is None:
        api_key = os.environ.get("GOOGLE_VISION_API_KEY")
    
    if not api_key:
        return {
            "full_text": "",
            "success": False,
            "confidence": 0,
            "error": "GOOGLE_VISION_API_KEY non configur√©e"
        }
    
    try:
        # Construire la requ√™te
        request_body = {
            "requests": [
                {
                    "image": {
                        "content": image_base64
                    },
                    "features": [
                        {
                            "type": "DOCUMENT_TEXT_DETECTION",
                            "maxResults": 50
                        }
                    ],
                    "imageContext": {
                        "languageHints": ["en", "fr"]
                    }
                }
            ]
        }
        
        headers = {
            "Content-Type": "application/json; charset=utf-8"
        }
        
        # Appel API
        response = requests.post(
            f"{GOOGLE_VISION_ENDPOINT}?key={api_key}",
            headers=headers,
            json=request_body,
            timeout=30
        )
        
        response.raise_for_status()
        result = response.json()
        
        # Parser la r√©ponse
        if "responses" not in result or len(result["responses"]) == 0:
            return {
                "full_text": "",
                "success": False,
                "confidence": 0,
                "error": "R√©ponse API vide"
            }
        
        response_data = result["responses"][0]
        
        # V√©rifier s'il y a une erreur dans la r√©ponse
        if "error" in response_data:
            error_msg = response_data["error"].get("message", "Erreur inconnue")
            return {
                "full_text": "",
                "success": False,
                "confidence": 0,
                "error": f"Google Vision API error: {error_msg}"
            }
        
        # Extraire le texte complet
        full_text = ""
        confidence = 0.0
        
        if "fullTextAnnotation" in response_data:
            full_text = response_data["fullTextAnnotation"].get("text", "")
            
            # Calculer la confiance moyenne depuis les pages
            pages = response_data["fullTextAnnotation"].get("pages", [])
            total_conf = 0
            count = 0
            for page in pages:
                for block in page.get("blocks", []):
                    if "confidence" in block:
                        total_conf += block["confidence"]
                        count += 1
            confidence = total_conf / count if count > 0 else 0.9
        
        elif "textAnnotations" in response_data and len(response_data["textAnnotations"]) > 0:
            # Fallback: utiliser la premi√®re annotation (texte complet)
            full_text = response_data["textAnnotations"][0].get("description", "")
            confidence = 0.85  # Confiance par d√©faut
        
        logger.info(f"Google Vision OCR: {len(full_text)} caract√®res extraits, confiance={confidence:.2f}")
        
        return {
            "full_text": full_text,
            "success": True,
            "confidence": confidence,
            "error": None
        }
        
    except requests.exceptions.Timeout:
        logger.error("Google Vision API timeout")
        return {
            "full_text": "",
            "success": False,
            "confidence": 0,
            "error": "Timeout - API Google Vision n'a pas r√©pondu"
        }
    except requests.exceptions.RequestException as e:
        logger.error(f"Google Vision API request error: {e}")
        return {
            "full_text": "",
            "success": False,
            "confidence": 0,
            "error": f"Erreur r√©seau: {str(e)}"
        }
    except Exception as e:
        logger.error(f"Google Vision API error: {e}")
        return {
            "full_text": "",
            "success": False,
            "confidence": 0,
            "error": f"Erreur inattendue: {str(e)}"
        }


def google_vision_ocr_from_bytes(image_bytes: bytes, api_key: Optional[str] = None) -> Dict[str, any]:
    """
    Wrapper pour appeler Google Vision OCR depuis des bytes d'image.
    
    Args:
        image_bytes: Image en bytes (JPEG, PNG, etc.)
        api_key: Cl√© API (optionnelle)
    
    Returns:
        R√©sultat OCR
    """
    # Encoder en base64
    image_base64 = base64.b64encode(image_bytes).decode("utf-8")
    return google_vision_ocr(image_base64, api_key)


def google_vision_ocr_from_numpy(cv_image: np.ndarray, api_key: Optional[str] = None) -> Dict[str, any]:
    """
    Wrapper pour appeler Google Vision OCR depuis une image numpy/OpenCV.
    
    Convertit l'image en JPEG avant l'envoi pour optimiser la taille.
    
    Args:
        cv_image: Image numpy (BGR ou Grayscale)
        api_key: Cl√© API (optionnelle)
    
    Returns:
        R√©sultat OCR
    """
    # Convertir en JPEG
    if len(cv_image.shape) == 2:
        # Grayscale - convertir en BGR pour l'encodage
        cv_image = cv2.cvtColor(cv_image, cv2.COLOR_GRAY2BGR)
    
    # Encoder en JPEG avec qualit√© optimale pour OCR
    _, buffer = cv2.imencode('.jpg', cv_image, [cv2.IMWRITE_JPEG_QUALITY, 95])
    image_bytes = buffer.tobytes()
    
    return google_vision_ocr_from_bytes(image_bytes, api_key)


# ============ CORRECTION PERSPECTIVE ============

def order_points(pts: np.ndarray) -> np.ndarray:
    """Ordonne les 4 points: top-left, top-right, bottom-right, bottom-left"""
    rect = np.zeros((4, 2), dtype="float32")
    s = pts.sum(axis=1)
    rect[0] = pts[np.argmin(s)]
    rect[2] = pts[np.argmax(s)]
    diff = np.diff(pts, axis=1)
    rect[1] = pts[np.argmin(diff)]
    rect[3] = pts[np.argmax(diff)]
    return rect


def four_point_transform(image: np.ndarray, pts: np.ndarray) -> np.ndarray:
    """Transformation perspective 4 points ‚Üí rectangle"""
    rect = order_points(pts)
    (tl, tr, br, bl) = rect
    
    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))
    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))
    maxWidth = max(int(widthA), int(widthB))
    
    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))
    maxHeight = max(int(heightA), int(heightB))
    
    dst = np.array([
        [0, 0],
        [maxWidth - 1, 0],
        [maxWidth - 1, maxHeight - 1],
        [0, maxHeight - 1]], dtype="float32")
    
    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))
    return warped


def auto_warp_document(image: np.ndarray) -> np.ndarray:
    """
    D√©tection automatique des contours + redressement du document
    
    90% des √©checs OCR viennent de la perspective.
    Cette fonction corrige automatiquement l'angle.
    
    AM√âLIOR√â: V√©rifie que le contour d√©tect√© repr√©sente au moins 30% de l'image
    """
    try:
        h, w = image.shape[:2]
        image_area = h * w
        min_contour_area = image_area * 0.3  # Le document doit faire au moins 30% de l'image
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # Essayer plusieurs seuils Canny pour trouver le meilleur contour
        best_contour = None
        best_area = 0
        
        for low_thresh in [30, 50, 75, 100]:
            high_thresh = low_thresh * 2.5
            edged = cv2.Canny(blur, low_thresh, high_thresh)
            
            # Dilater pour connecter les lignes bris√©es
            kernel = np.ones((3, 3), np.uint8)
            edged = cv2.dilate(edged, kernel, iterations=1)
            
            contours, _ = cv2.findContours(edged, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)
            contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]
            
            for c in contours:
                area = cv2.contourArea(c)
                if area < min_contour_area:
                    continue
                    
                peri = cv2.arcLength(c, True)
                approx = cv2.approxPolyDP(c, 0.02 * peri, True)
                
                if len(approx) == 4 and area > best_area:
                    best_contour = approx
                    best_area = area
        
        if best_contour is not None:
            pts = best_contour.reshape(4, 2).astype("float32")
            logger.info(f"Document contour detected (area={best_area/image_area*100:.1f}%), applying perspective correction")
            return four_point_transform(image, pts)
        
        logger.info("No valid document contour found, using original image (no perspective correction)")
        return image
        
    except Exception as e:
        logger.warning(f"Auto-warp failed: {e}, using original image")
        return image


# ============ PREPROCESSING OCR ============

def remove_shadows(image: np.ndarray) -> np.ndarray:
    """
    Suppression des ombres - Technique CamScanner
    
    Divise l'image en blocs et normalise l'√©clairage local
    pour obtenir un fond blanc uniforme.
    """
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image.copy()
    
    # Cr√©er un fond estim√© avec un gros flou
    # Plus le kernel est grand, plus les ombres larges sont supprim√©es
    kernel_size = max(gray.shape[0], gray.shape[1]) // 8
    if kernel_size % 2 == 0:
        kernel_size += 1
    kernel_size = max(kernel_size, 51)  # Minimum 51
    
    # Filtre morphologique pour estimer le fond
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
    background = cv2.morphologyEx(gray, cv2.MORPH_DILATE, kernel)
    
    # Diviser l'image par le fond pour normaliser l'√©clairage
    # √âviter division par z√©ro
    background = np.maximum(background, 1)
    normalized = cv2.divide(gray, background, scale=255)
    
    return normalized


def enhance_contrast_adaptive(image: np.ndarray) -> np.ndarray:
    """
    Am√©lioration du contraste avec CLAHE
    (Contrast Limited Adaptive Histogram Equalization)
    
    Meilleur que l'√©galisation standard car √©vite la sur-amplification.
    """
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image.copy()
    
    # CLAHE avec clip limit mod√©r√©
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)
    
    return enhanced


def adaptive_binarization(image: np.ndarray) -> np.ndarray:
    """
    Binarisation adaptative - Texte noir, fond blanc
    
    Utilise le seuillage adaptatif gaussien qui est
    meilleur que Otsu pour les documents avec √©clairage in√©gal.
    """
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    else:
        gray = image.copy()
    
    # Seuillage adaptatif - meilleur pour documents
    binary = cv2.adaptiveThreshold(
        gray, 
        255, 
        cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
        cv2.THRESH_BINARY,
        blockSize=21,  # Taille du bloc local
        C=10  # Constante soustraite de la moyenne
    )
    
    return binary


def clean_document_edges(image: np.ndarray) -> np.ndarray:
    """
    Nettoie les bords du document (supprime le bruit de bordure)
    """
    h, w = image.shape[:2]
    
    # Cr√©er un masque avec bordure noire (5% de chaque c√¥t√©)
    border = int(min(h, w) * 0.02)
    mask = np.zeros((h, w), dtype=np.uint8)
    mask[border:h-border, border:w-border] = 255
    
    # Appliquer le masque (fond blanc √† l'ext√©rieur)
    if len(image.shape) == 3:
        result = cv2.bitwise_and(image, image, mask=mask)
        result[mask == 0] = 255
    else:
        result = image.copy()
        result[mask == 0] = 255
    
    return result


def camscanner_preprocess(image: np.ndarray) -> np.ndarray:
    """
    üéØ PR√âTRAITEMENT STYLE CAMSCANNER - Pipeline complet
    
    Transforme une photo de document en scan propre:
    1. D√©tection et correction de perspective
    2. Suppression des ombres
    3. Am√©lioration du contraste
    4. Binarisation adaptative
    5. Nettoyage final
    
    Input: Image BGR (photo de document)
    Output: Image binaire optimis√©e pour OCR (noir sur blanc)
    """
    logger.info("CamScanner preprocess: D√©marrage du pipeline")
    
    # √âtape 1: Redimensionner si trop grand
    h, w = image.shape[:2]
    max_dim = 2000
    if max(h, w) > max_dim:
        scale = max_dim / max(h, w)
        image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)
        logger.info(f"  Redimensionn√©: {w}x{h} ‚Üí {image.shape[1]}x{image.shape[0]}")
    
    # √âtape 2: Correction de perspective (redressement)
    warped = auto_warp_document(image)
    logger.info(f"  Perspective corrig√©e: {warped.shape[1]}x{warped.shape[0]}")
    
    # √âtape 3: Conversion en niveaux de gris
    if len(warped.shape) == 3:
        gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)
    else:
        gray = warped.copy()
    
    # √âtape 4: Suppression des ombres
    no_shadows = remove_shadows(gray)
    logger.info("  Ombres supprim√©es")
    
    # √âtape 5: Am√©lioration du contraste (CLAHE)
    enhanced = enhance_contrast_adaptive(no_shadows)
    logger.info("  Contraste am√©lior√©")
    
    # √âtape 6: D√©bruitage l√©ger
    denoised = cv2.fastNlMeansDenoising(enhanced, h=8)
    
    # √âtape 7: Binarisation adaptative (texte noir, fond blanc)
    binary = adaptive_binarization(denoised)
    logger.info("  Binarisation appliqu√©e")
    
    # √âtape 8: Nettoyage morphologique (supprime petits bruits)
    kernel = np.ones((2, 2), np.uint8)
    cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
    cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, kernel)
    
    # √âtape 9: Nettoyage des bords
    final = clean_document_edges(cleaned)
    logger.info("  Nettoyage termin√©")
    
    logger.info("CamScanner preprocess: Pipeline termin√© avec succ√®s")
    
    return final


def camscanner_preprocess_for_vision(image: np.ndarray) -> np.ndarray:
    """
    Version du pr√©traitement optimis√©e pour GPT-4 Vision.
    
    GPT-4 Vision pr√©f√®re des images avec un peu de contexte,
    donc on garde les niveaux de gris au lieu de binaire pur.
    """
    logger.info("CamScanner preprocess (Vision): D√©marrage")
    
    # Redimensionner si trop grand
    h, w = image.shape[:2]
    max_dim = 2000
    if max(h, w) > max_dim:
        scale = max_dim / max(h, w)
        image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)
    
    # Correction de perspective
    warped = auto_warp_document(image)
    
    # Conversion en niveaux de gris
    if len(warped.shape) == 3:
        gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)
    else:
        gray = warped.copy()
    
    # Suppression des ombres
    no_shadows = remove_shadows(gray)
    
    # Am√©lioration du contraste
    enhanced = enhance_contrast_adaptive(no_shadows)
    
    # D√©bruitage
    denoised = cv2.fastNlMeansDenoising(enhanced, h=8)
    
    # Pour Vision: on garde en niveaux de gris avec contraste am√©lior√©
    # (pas de binarisation compl√®te)
    
    # Augmenter le contraste final
    # √âtirer l'histogramme pour maximiser le contraste
    min_val = np.percentile(denoised, 2)
    max_val = np.percentile(denoised, 98)
    
    # √âviter division par z√©ro
    if max_val - min_val < 1:
        stretched = denoised
    else:
        stretched = np.clip((denoised.astype(np.float32) - min_val) * 255 / (max_val - min_val), 0, 255).astype(np.uint8)
    
    logger.info("CamScanner preprocess (Vision): Termin√©")
    
    return stretched


def preprocess_for_ocr(zone_img: np.ndarray) -> np.ndarray:
    """
    Pr√©traitement intelligent avant OCR:
    - Conversion grayscale
    - D√©bruitage
    - Binarisation Otsu
    
    Tesseract est mauvais avec colonnes multiples,
    mais bon avec texte isol√© + pr√©trait√©.
    """
    if len(zone_img.shape) == 3:
        gray = cv2.cvtColor(zone_img, cv2.COLOR_BGR2GRAY)
    else:
        gray = zone_img.copy()
    
    # D√©bruitage l√©ger
    denoised = cv2.fastNlMeansDenoising(gray, h=10)
    
    # Binarisation avec Otsu (meilleur que seuillage adaptatif pour photos)
    _, binary = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    return binary


# ============ SEGMENTATION ROI ============

def extract_zones(image: np.ndarray) -> Dict[str, np.ndarray]:
    """
    D√©coupe la facture FCA en zones (ROI - Region of Interest)
    
    Structure des factures FCA Canada:
    - Haut droite ‚Üí VIN + Model code
    - Centre ‚Üí Options (liste des √©quipements)
    - Bas gauche ‚Üí Codes financiers (EP, PDCO, PREF)
    - Bas droite ‚Üí Totaux (Subtotal, Total)
    
    OCR par zones = beaucoup plus propre que OCR global
    """
    h, w = image.shape[:2]
    
    zones = {
        # Zone VIN: haut droite (0-35% hauteur, 40-100% largeur)
        "vin": image[0:int(h*0.35), int(w*0.4):w],
        
        # Zone options: centre (30-75% hauteur, toute largeur)
        "options": image[int(h*0.30):int(h*0.75), :],
        
        # Zone financi√®re: BAS GAUCHE (65-95% hauteur, 0-50% largeur)
        # C'est l√† que EP, PDCO, PREF sont situ√©s
        "finance": image[int(h*0.65):int(h*0.95), 0:int(w*0.5)],
        
        # Zone totaux: bas droite (70-100% hauteur, 50-100% largeur)
        "totals": image[int(h*0.70):h, int(w*0.5):w]
    }
    
    return zones


# ============ OCR CIBL√â PAR ZONE ============

def ocr_zone(zone_img: np.ndarray, lang: str = "eng+fra", psm: int = 6) -> str:
    """
    OCR cibl√© sur une zone pr√©trait√©e
    
    PSM modes:
    - 6: Assume a single uniform block of text (d√©faut pour zones)
    - 7: Treat the image as a single text line (meilleur pour VIN)
    - 4: Assume a single column of text
    - 11: Sparse text
    """
    try:
        processed = preprocess_for_ocr(zone_img)
        text = pytesseract.image_to_string(
            processed,
            lang=lang,
            config=f"--psm {psm} --oem 3"
        )
        return text.strip()
    except Exception as e:
        logger.error(f"OCR zone error: {e}")
        return ""


# ============ PIPELINE COMPLET ============

def load_image_from_bytes(file_bytes: bytes) -> Optional[np.ndarray]:
    """Charge une image depuis bytes"""
    try:
        nparr = np.frombuffer(file_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        return image
    except Exception as e:
        logger.error(f"Failed to load image: {e}")
        return None


def resize_if_needed(image: np.ndarray, max_dim: int = 2500) -> np.ndarray:
    """Redimensionne si l'image est trop grande (√©conomise m√©moire)"""
    h, w = image.shape[:2]
    if max(h, w) > max_dim:
        scale = max_dim / max(h, w)
        image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)
        logger.info(f"Image resized from {w}x{h} to {image.shape[1]}x{image.shape[0]}")
    return image


def process_image_ocr_pipeline(file_bytes: bytes) -> Dict[str, str]:
    """
    Pipeline OCR complet par zones:
    
    Image ‚Üí Load ‚Üí Resize ‚Üí Warp ‚Üí Zones ‚Üí OCR cibl√©
    
    Si les zones √©chouent, fallback sur OCR global.
    
    Returns: Dict avec le texte de chaque zone
    """
    result = {
        "vin_text": "",
        "finance_text": "",
        "options_text": "",
        "totals_text": "",
        "full_text": "",
        "zones_processed": 0,
        "parse_method": "ocr_zones"
    }
    
    # 1. Charger l'image
    image = load_image_from_bytes(file_bytes)
    if image is None:
        logger.error("Failed to decode image")
        return result
    
    # 2. Redimensionner - taille optimale pour OCR
    image = resize_if_needed(image, max_dim=1800)
    h, w = image.shape[:2]
    
    # 3. Correction perspective (redresser le document)
    warped = auto_warp_document(image)
    
    # 4. Extraire les zones
    zones = extract_zones(warped)
    
    # 5. OCR sur chaque zone avec PSM optimis√©
    
    # Zone VIN: utiliser PSM=7 (single line) pour meilleure pr√©cision VIN
    result["vin_text"] = ocr_zone(zones["vin"], psm=7)
    
    # AM√âLIORATION: Fallback avec zone √©largie si VIN trop court
    if len(result["vin_text"]) < 10:
        logger.info("VIN zone trop courte, √©largissement de la zone")
        enlarged_vin_zone = warped[0:int(h*0.45), int(w*0.3):w]
        result["vin_text"] = ocr_zone(enlarged_vin_zone, psm=7)
    
    if result["vin_text"] and len(result["vin_text"]) > 10:
        result["zones_processed"] += 1
    
    # Zone finance: PSM=6 (block)
    result["finance_text"] = ocr_zone(zones["finance"], psm=6)
    if result["finance_text"] and len(result["finance_text"]) > 10:
        result["zones_processed"] += 1
    
    # Zone options: PSM=6 (block)
    result["options_text"] = ocr_zone(zones["options"], psm=6)
    if result["options_text"] and len(result["options_text"]) > 10:
        result["zones_processed"] += 1
    
    # Zone totaux: PSM=6 (block)
    result["totals_text"] = ocr_zone(zones["totals"], psm=6)
    if result["totals_text"] and len(result["totals_text"]) > 10:
        result["zones_processed"] += 1
    
    # 6. AM√âLIORATION: OCR global seulement si zones insuffisantes
    if result["zones_processed"] >= 2:
        # Zones suffisantes, pas besoin de global (√©vite duplication/bruit)
        result["full_text"] = "\n".join([
            result["vin_text"],
            result["finance_text"],
            result["options_text"],
            result["totals_text"]
        ])
        logger.info(f"Using zones only (zones={result['zones_processed']})")
    else:
        # Zones insuffisantes, ajouter OCR global
        global_text = process_image_global_ocr(file_bytes)
        result["full_text"] = global_text
        result["parse_method"] = "ocr_global"
        logger.info(f"Using global OCR fallback (zones={result['zones_processed']}, global_len={len(global_text)})")
    
    logger.info(f"OCR Pipeline: {result['zones_processed']}/4 zones processed")
    
    return result


def process_image_global_ocr(file_bytes: bytes) -> str:
    """
    OCR global sur toute l'image (fallback si ROI ne fonctionne pas)
    Utilise un pr√©traitement optimis√© pour les photos de factures
    """
    try:
        image = load_image_from_bytes(file_bytes)
        if image is None:
            return ""
        
        # Redimensionner √† taille optimale pour OCR
        image = resize_if_needed(image, max_dim=1800)
        
        # Convertir en grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # D√©bruitage
        denoised = cv2.fastNlMeansDenoising(gray, h=10)
        
        # Binarisation Otsu
        _, binary = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # OCR avec config optimis√©e pour documents
        text = pytesseract.image_to_string(
            binary,
            lang="eng+fra",
            config="--oem 3 --psm 6"
        )
        
        return text.strip()
        
    except Exception as e:
        logger.error(f"Global OCR error: {e}")
        return ""
